{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "sentiment_classifier = pipeline(\"sentiment-analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997096657752991}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_classifier(\"I'm so excited to be learning about large language models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sasi virat\\.cache\\huggingface\\hub\\models--dslim--bert-base-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", model = \"dslim/bert-base-NER\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sasi virat\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model = \"facebook/bart-large-mnli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['travel', 'dancing', 'cooking'],\n",
       " 'scores': [0.9938651323318481, 0.0032737741712480783, 0.0028610252775251865]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroshot_classifier(sequence_to_classify, candidate_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"bert-base-uncased\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sasi virat\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I'm so excited to be learning about large language models\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 1005, 1049, 2061, 7568, 2000, 2022, 4083, 2055, 2312, 2653, 4275, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(sentence)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'm', 'so', 'excited', 'to', 'be', 'learning', 'about', 'large', 'language', 'models']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 1005, 1049, 2061, 7568, 2000, 2022, 4083, 2055, 2312, 2653, 4275]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i ' m so excited to be learning about large language models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "decoded_ids = tokenizer.decode(token_ids)\n",
    "print(decoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(102)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = \"xlnet-base-cased\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sasi virat\\.cache\\huggingface\\hub\\models--xlnet-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer2(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [35, 26, 98, 102, 5564, 22, 39, 1899, 75, 392, 1243, 2626, 4, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁I', \"'\", 'm', '▁so', '▁excited', '▁to', '▁be', '▁learning', '▁about', '▁large', '▁language', '▁models']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer2.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 26, 98, 102, 5564, 22, 39, 1899, 75, 392, 1243, 2626]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer2.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sep>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface and Pytorch/Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\sasi virat\\anaconda3\\envs\\dl_env\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sasi virat\\anaconda3\\envs\\dl_env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sasi virat\\anaconda3\\envs\\dl_env\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sasi virat\\anaconda3\\envs\\dl_env\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sasi virat\\anaconda3\\envs\\dl_env\\lib\\site-packages (from torch) (75.6.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sasi virat\\anaconda3\\envs\\dl_env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "   ---------------------------------------- 0.0/203.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.9/203.0 MB 18.6 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 7.1/203.0 MB 18.9 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 10.0/203.0 MB 16.8 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 14.7/203.0 MB 18.1 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 18.1/203.0 MB 17.8 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 18.6/203.0 MB 14.9 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 21.0/203.0 MB 14.7 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 23.1/203.0 MB 13.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 24.1/203.0 MB 12.7 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 25.4/203.0 MB 12.2 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 27.0/203.0 MB 11.9 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 28.6/203.0 MB 11.6 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 30.7/203.0 MB 11.2 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 33.3/203.0 MB 11.2 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 35.4/203.0 MB 11.1 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 36.7/203.0 MB 10.8 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 38.3/203.0 MB 10.7 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 40.6/203.0 MB 10.7 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 42.2/203.0 MB 10.4 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 44.3/203.0 MB 10.4 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 46.1/203.0 MB 10.3 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 47.4/203.0 MB 10.1 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 50.6/203.0 MB 10.3 MB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 54.0/203.0 MB 10.5 MB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 57.9/203.0 MB 10.8 MB/s eta 0:00:14\n",
      "   ------------ --------------------------- 61.3/203.0 MB 11.0 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 65.5/203.0 MB 11.4 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 70.3/203.0 MB 11.7 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 74.7/203.0 MB 12.0 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 79.4/203.0 MB 12.4 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 81.5/203.0 MB 12.5 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 81.5/203.0 MB 12.5 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 81.8/203.0 MB 11.6 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.1/203.0 MB 11.6 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.3/203.0 MB 11.0 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.3/203.0 MB 11.0 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.3/203.0 MB 11.0 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.3/203.0 MB 11.0 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.3/203.0 MB 11.0 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.3/203.0 MB 11.0 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.3/203.0 MB 11.0 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.3/203.0 MB 11.0 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.3/203.0 MB 11.0 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.3/203.0 MB 11.0 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.3/203.0 MB 11.0 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.3/203.0 MB 11.0 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 83.4/203.0 MB 8.3 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 83.6/203.0 MB 8.3 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 83.6/203.0 MB 8.3 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 83.6/203.0 MB 8.3 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 83.6/203.0 MB 8.3 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 84.7/203.0 MB 7.7 MB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 85.7/203.0 MB 7.6 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 88.3/203.0 MB 7.7 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 90.7/203.0 MB 7.7 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 91.0/203.0 MB 7.7 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 91.5/203.0 MB 7.6 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 92.5/203.0 MB 7.5 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 93.1/203.0 MB 7.5 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 94.1/203.0 MB 7.4 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 94.9/203.0 MB 7.4 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 95.4/203.0 MB 7.3 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 96.2/203.0 MB 7.2 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 96.2/203.0 MB 7.2 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 96.5/203.0 MB 7.1 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 96.5/203.0 MB 7.1 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 96.7/203.0 MB 6.8 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 97.0/203.0 MB 6.8 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 97.0/203.0 MB 6.8 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 97.3/203.0 MB 6.6 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 97.8/203.0 MB 6.5 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 98.0/203.0 MB 6.4 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 98.3/203.0 MB 6.4 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 98.6/203.0 MB 6.3 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 98.8/203.0 MB 6.2 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 98.8/203.0 MB 6.2 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 99.4/203.0 MB 6.1 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 99.9/203.0 MB 6.0 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 100.4/203.0 MB 6.0 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 100.7/203.0 MB 6.0 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 101.2/203.0 MB 5.9 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 101.2/203.0 MB 5.9 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 101.2/203.0 MB 5.9 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 101.2/203.0 MB 5.9 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 101.4/203.0 MB 5.6 MB/s eta 0:00:19\n",
      "   ------------------- -------------------- 101.4/203.0 MB 5.6 MB/s eta 0:00:19\n",
      "   ------------------- -------------------- 101.4/203.0 MB 5.6 MB/s eta 0:00:19\n",
      "   ------------------- -------------------- 101.4/203.0 MB 5.6 MB/s eta 0:00:19\n",
      "   -------------------- ------------------- 101.7/203.0 MB 5.4 MB/s eta 0:00:19\n",
      "   -------------------- ------------------- 101.7/203.0 MB 5.4 MB/s eta 0:00:19\n",
      "   -------------------- ------------------- 102.0/203.0 MB 5.3 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 102.0/203.0 MB 5.3 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 102.0/203.0 MB 5.3 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 102.0/203.0 MB 5.3 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 102.0/203.0 MB 5.3 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 102.0/203.0 MB 5.3 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 102.0/203.0 MB 5.3 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 102.0/203.0 MB 5.3 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 102.0/203.0 MB 5.3 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 102.0/203.0 MB 5.3 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 102.0/203.0 MB 5.3 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 102.0/203.0 MB 5.3 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 102.2/203.0 MB 4.7 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 102.2/203.0 MB 4.7 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 102.2/203.0 MB 4.7 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 102.5/203.0 MB 4.5 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 102.5/203.0 MB 4.5 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 102.5/203.0 MB 4.5 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 102.8/203.0 MB 4.5 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 102.8/203.0 MB 4.5 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 103.0/203.0 MB 4.4 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 103.0/203.0 MB 4.4 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 103.0/203.0 MB 4.4 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 103.0/203.0 MB 4.4 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 103.0/203.0 MB 4.4 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 103.3/203.0 MB 4.2 MB/s eta 0:00:24\n",
      "   -------------------- ------------------- 103.3/203.0 MB 4.2 MB/s eta 0:00:24\n",
      "   -------------------- ------------------- 103.5/203.0 MB 4.1 MB/s eta 0:00:25\n",
      "   -------------------- ------------------- 103.5/203.0 MB 4.1 MB/s eta 0:00:25\n",
      "   -------------------- ------------------- 103.8/203.0 MB 4.1 MB/s eta 0:00:25\n",
      "   -------------------- ------------------- 104.3/203.0 MB 4.1 MB/s eta 0:00:25\n",
      "   -------------------- ------------------- 105.4/203.0 MB 4.1 MB/s eta 0:00:25\n",
      "   --------------------- ------------------ 106.7/203.0 MB 4.1 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 107.0/203.0 MB 4.1 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 107.0/203.0 MB 4.1 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 107.0/203.0 MB 4.1 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 107.0/203.0 MB 4.1 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 107.2/203.0 MB 3.9 MB/s eta 0:00:25\n",
      "   --------------------- ------------------ 108.3/203.0 MB 3.9 MB/s eta 0:00:25\n",
      "   --------------------- ------------------ 108.5/203.0 MB 3.9 MB/s eta 0:00:25\n",
      "   --------------------- ------------------ 109.1/203.0 MB 3.9 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 109.8/203.0 MB 3.9 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 110.4/203.0 MB 3.9 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 110.6/203.0 MB 3.9 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 110.9/203.0 MB 3.9 MB/s eta 0:00:24\n",
      "   --------------------- ------------------ 111.4/203.0 MB 3.8 MB/s eta 0:00:24\n",
      "   ---------------------- ----------------- 111.9/203.0 MB 3.8 MB/s eta 0:00:24\n",
      "   ---------------------- ----------------- 112.5/203.0 MB 3.8 MB/s eta 0:00:24\n",
      "   ---------------------- ----------------- 112.7/203.0 MB 3.8 MB/s eta 0:00:24\n",
      "   ---------------------- ----------------- 113.2/203.0 MB 3.8 MB/s eta 0:00:24\n",
      "   ---------------------- ----------------- 114.3/203.0 MB 3.8 MB/s eta 0:00:24\n",
      "   ---------------------- ----------------- 115.6/203.0 MB 3.7 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 116.9/203.0 MB 3.6 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 118.2/203.0 MB 3.6 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 119.8/203.0 MB 3.5 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 121.1/203.0 MB 3.4 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 121.1/203.0 MB 3.4 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 121.1/203.0 MB 3.4 MB/s eta 0:00:24\n",
      "   ----------------------- ---------------- 121.6/203.0 MB 3.3 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 122.4/203.0 MB 3.3 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 122.9/203.0 MB 3.3 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 123.7/203.0 MB 3.2 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 124.5/203.0 MB 3.2 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 125.3/203.0 MB 3.1 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 125.8/203.0 MB 3.1 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 126.6/203.0 MB 3.0 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 127.4/203.0 MB 3.0 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 127.9/203.0 MB 3.0 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 128.2/203.0 MB 2.9 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 128.7/203.0 MB 2.9 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 129.0/203.0 MB 2.9 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 129.5/203.0 MB 2.8 MB/s eta 0:00:27\n",
      "   ------------------------- -------------- 130.0/203.0 MB 2.8 MB/s eta 0:00:27\n",
      "   ------------------------- -------------- 130.5/203.0 MB 2.7 MB/s eta 0:00:27\n",
      "   ------------------------- -------------- 131.3/203.0 MB 2.6 MB/s eta 0:00:29\n",
      "   -------------------------- ------------- 132.1/203.0 MB 2.5 MB/s eta 0:00:29\n",
      "   -------------------------- ------------- 132.6/203.0 MB 2.4 MB/s eta 0:00:30\n",
      "   -------------------------- ------------- 133.7/203.0 MB 2.2 MB/s eta 0:00:31\n",
      "   -------------------------- ------------- 134.5/203.0 MB 2.1 MB/s eta 0:00:33\n",
      "   -------------------------- ------------- 135.3/203.0 MB 2.0 MB/s eta 0:00:34\n",
      "   -------------------------- ------------- 135.8/203.0 MB 1.9 MB/s eta 0:00:36\n",
      "   -------------------------- ------------- 136.6/203.0 MB 1.9 MB/s eta 0:00:36\n",
      "   --------------------------- ------------ 137.1/203.0 MB 1.9 MB/s eta 0:00:36\n",
      "   --------------------------- ------------ 137.6/203.0 MB 1.9 MB/s eta 0:00:36\n",
      "   --------------------------- ------------ 137.9/203.0 MB 1.9 MB/s eta 0:00:35\n",
      "   --------------------------- ------------ 138.1/203.0 MB 1.9 MB/s eta 0:00:35\n",
      "   --------------------------- ------------ 138.9/203.0 MB 2.0 MB/s eta 0:00:32\n",
      "   --------------------------- ------------ 139.2/203.0 MB 2.0 MB/s eta 0:00:32\n",
      "   --------------------------- ------------ 139.7/203.0 MB 2.0 MB/s eta 0:00:32\n",
      "   --------------------------- ------------ 140.5/203.0 MB 2.0 MB/s eta 0:00:31\n",
      "   --------------------------- ------------ 141.0/203.0 MB 2.0 MB/s eta 0:00:31\n",
      "   --------------------------- ------------ 141.6/203.0 MB 2.0 MB/s eta 0:00:31\n",
      "   ---------------------------- ----------- 142.3/203.0 MB 2.1 MB/s eta 0:00:30\n",
      "   ---------------------------- ----------- 143.1/203.0 MB 2.1 MB/s eta 0:00:29\n",
      "   ---------------------------- ----------- 144.2/203.0 MB 2.1 MB/s eta 0:00:29\n",
      "   ---------------------------- ----------- 145.0/203.0 MB 2.1 MB/s eta 0:00:28\n",
      "   ---------------------------- ----------- 145.8/203.0 MB 2.1 MB/s eta 0:00:28\n",
      "   ---------------------------- ----------- 146.3/203.0 MB 2.1 MB/s eta 0:00:27\n",
      "   ---------------------------- ----------- 147.1/203.0 MB 2.2 MB/s eta 0:00:26\n",
      "   ----------------------------- ---------- 147.8/203.0 MB 2.2 MB/s eta 0:00:26\n",
      "   ----------------------------- ---------- 148.4/203.0 MB 2.2 MB/s eta 0:00:26\n",
      "   ----------------------------- ---------- 148.6/203.0 MB 2.2 MB/s eta 0:00:26\n",
      "   ----------------------------- ---------- 149.2/203.0 MB 2.1 MB/s eta 0:00:26\n",
      "   ----------------------------- ---------- 149.7/203.0 MB 2.1 MB/s eta 0:00:26\n",
      "   ----------------------------- ---------- 150.2/203.0 MB 2.0 MB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 151.0/203.0 MB 2.0 MB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 151.8/203.0 MB 2.0 MB/s eta 0:00:26\n",
      "   ------------------------------ --------- 152.8/203.0 MB 2.0 MB/s eta 0:00:25\n",
      "   ------------------------------ --------- 153.9/203.0 MB 2.0 MB/s eta 0:00:25\n",
      "   ------------------------------ --------- 155.2/203.0 MB 2.0 MB/s eta 0:00:24\n",
      "   ------------------------------ --------- 156.5/203.0 MB 2.1 MB/s eta 0:00:23\n",
      "   ------------------------------- -------- 157.5/203.0 MB 2.1 MB/s eta 0:00:22\n",
      "   ------------------------------- -------- 158.9/203.0 MB 2.1 MB/s eta 0:00:22\n",
      "   ------------------------------- -------- 160.2/203.0 MB 2.1 MB/s eta 0:00:21\n",
      "   ------------------------------- -------- 161.7/203.0 MB 2.2 MB/s eta 0:00:19\n",
      "   -------------------------------- ------- 163.1/203.0 MB 2.2 MB/s eta 0:00:18\n",
      "   -------------------------------- ------- 164.6/203.0 MB 2.3 MB/s eta 0:00:17\n",
      "   -------------------------------- ------- 166.5/203.0 MB 2.3 MB/s eta 0:00:16\n",
      "   --------------------------------- ------ 168.0/203.0 MB 2.4 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 169.6/203.0 MB 2.4 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 171.4/203.0 MB 2.5 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 173.3/203.0 MB 2.5 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 175.4/203.0 MB 2.6 MB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 177.7/203.0 MB 2.7 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 180.1/203.0 MB 2.7 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 182.2/203.0 MB 2.8 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 184.3/203.0 MB 2.8 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 186.9/203.0 MB 2.9 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 189.3/203.0 MB 3.0 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 191.6/203.0 MB 3.0 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 194.0/203.0 MB 3.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 196.1/203.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------------------  198.7/203.0 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------------------------  201.3/203.0 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.9/203.0 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 203.0/203.0 MB 3.4 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 2.9/6.2 MB 14.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 12.2 MB/s eta 0:00:00\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 11.7 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 8.8 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm so excited to be learning about large language models\n",
      "{'input_ids': [35, 26, 98, 102, 5564, 22, 39, 1899, 75, 392, 1243, 2626, 4, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sentence)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sasi virat\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to convert output to PyTorch tensors format, PyTorch is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_ids_pt \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids_pt)\n",
      "File \u001b[1;32mc:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3021\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   3020\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 3021\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3023\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3131\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   3110\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3111\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3128\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3129\u001b[0m     )\n\u001b[0;32m   3130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3134\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3150\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3151\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3207\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3197\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3198\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3199\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3200\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3204\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3205\u001b[0m )\n\u001b[1;32m-> 3207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3210\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3226\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:603\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    581\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    601\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[0;32m    602\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[1;32m--> 603\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[1;32mc:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:577\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m sanitized_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[1;32m--> 577\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msanitized_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_encodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:240\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    236\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:730\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tensor_type \u001b[38;5;241m==\u001b[39m TensorType\u001b[38;5;241m.\u001b[39mPYTORCH:\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 730\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to convert output to PyTorch tensors format, PyTorch is not installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     is_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_tensor\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to convert output to PyTorch tensors format, PyTorch is not installed."
     ]
    }
   ],
   "source": [
    "input_ids_pt = tokenizer(sentence, return_tensors =\"pt\")\n",
    "print(input_ids_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForSequenceClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased-finetuned-sst-2-english\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1651\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1651\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sasi virat\\anaconda3\\envs\\DL_env\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1630\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1628\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[1;32m-> 1630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForSequenceClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids_pt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 2\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43minput_ids_pt\u001b[49m)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m      4\u001b[0m predicted_class_id \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mid2label[predicted_class_id]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_ids_pt' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**input_ids_pt).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3130509797.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[36], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Saving and loading models\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Saving and loading models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"my_saved_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(model_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = AutoTokenizer.from_pretrained(model_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = AutoModelForSequenceClassification.from_pretrained(model_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
